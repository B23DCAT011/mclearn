{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0728549-c96a-43d5-a7ba-243d39c19b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in V.\n",
    "    each column of V is a set of scores.\n",
    "    Z: a numpy array of shape (N, C)\n",
    "    return a numpy array of shape (N, C)\n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z)\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "def softmax_stable(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in Z.\n",
    "    each row of Z is a set of scores.\n",
    "    \"\"\"\n",
    "    # Z = Z.reshape(Z.shape[0], -1)\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c5a723-9424-43f1-8f70-7e17102f1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(X, y, W):\n",
    "    \"\"\"\n",
    "    W: 2d numpy array of shape (d, C),\n",
    "    each column correspoding to one output node\n",
    "    X: 2d numpy array of shape (N, d), each row is one data point\n",
    "    y: 1d numpy array -- label of each row of X\n",
    "    \"\"\"\n",
    "    A = softmax_stable(X.dot(W))\n",
    "    id0 = range(X.shape[0]) # indexes in axis 0, indexes in axis 1 are in y\n",
    "    return -np.mean(np.log(A[id0, y]))\n",
    "def softmax_grad(X, y, W):\n",
    "    \"\"\"\n",
    "    W: 2d numpy array of shape (d, C),\n",
    "    each column correspoding to one output node\n",
    "    X: 2d numpy array of shape (N, d), each row is one data point\n",
    "    y: 1d numpy array -- label of each row of X\n",
    "    \"\"\"\n",
    "    A = softmax_stable(X.dot(W)) # shape of (N, C)\n",
    "    id0 = range(X.shape[0])\n",
    "    A[id0, y] -= 1 # A - Y, shape of (N, C)\n",
    "    return X.T.dot(A)/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6335496-3104-44dc-a084-5d8ae30f0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fit(X, y, W, lr = 0.01, nepoches = 100, tol = 1e-5, batch_size = 10):\n",
    "    W_old = W.copy()\n",
    "    ep = 0\n",
    "    loss_hist = [softmax_loss(X, y, W)] # store history of loss\n",
    "    N = X.shape[0]\n",
    "    nbatches = int(np.ceil(float(N)/batch_size))\n",
    "    while ep < nepoches:\n",
    "        ep += 1\n",
    "        mix_ids = np.random.permutation(N) # mix data\n",
    "        for i in range(nbatches):\n",
    "            # get the i-th batch\n",
    "            batch_ids = mix_ids[batch_size*i:min(batch_size*(i+1), N)]\n",
    "            X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "            W -= lr*softmax_grad(X_batch, y_batch, W) # update gradient descent\n",
    "        loss_hist.append(softmax_loss(X, y, W))\n",
    "        if np.linalg.norm(W - W_old)/W.size < tol:\n",
    "            break\n",
    "        W_old = W.copy()\n",
    "    return W, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42676de3-f25b-4404-9fd1-b3a6cc4d1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(W, X):\n",
    "    \"\"\"\n",
    "    predict output of each columns of X . Class of each x_i is determined by\n",
    "    location of max probability. Note that classes are indexed from 0.\n",
    "    \"\"\"\n",
    "    return np.argmax(X.dot(W), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c5cd3c0-badf-4fb4-9f76-4616326eb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, N = 5, 500 # number of classes and number of points per class\n",
    "means = [[2, 2], [8, 3], [3, 6], [14, 2], [12, 8]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "X3 = np.random.multivariate_normal(means[3], cov, N)\n",
    "X4 = np.random.multivariate_normal(means[4], cov, N)\n",
    "X = np.concatenate((X0, X1, X2, X3, X4), axis = 0) # each row is a datapoint\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis = 1) # bias trick\n",
    "y = np.asarray([0]*N + [1]*N + [2]*N+ [3]*N + [4]*N)\n",
    "W_init = np.random.randn(Xbar.shape[1], C)\n",
    "W, loss_hist = softmax_fit(Xbar, y, W_init, batch_size = 10, nepoches = 100, lr =\n",
    "0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "481aafec-89b6-417f-9cc5-59f0e440f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.323107456523601, 0.6043749154838903, 0.47132442013315956, 0.3961944080504211, 0.3459797095361791, 0.3088092606529686, 0.2727758584556505, 0.2602868978496584, 0.24140040723762904, 0.23975800353022741, 0.2143791720625992, 0.21985389467364475, 0.19440898638008774, 0.19588365794502496, 0.18082177899409113, 0.3275755358559046, 0.16792216786151845, 0.1737137365883429, 0.16544352727966044, 0.15760388659129454, 0.16917696644702937, 0.14707309158234724, 0.15537645489755564, 0.1499557980580345, 0.13727747316177757, 0.1501671770033077, 0.13999973783067957, 0.14605914795705585, 0.1438449233056143, 0.12717627703609086, 0.13082150503617723, 0.12704034479770912, 0.11769295200031779, 0.12089125954361547, 0.12218928472306147, 0.11889602634390417, 0.15444410524513102, 0.12856316512431662, 0.11005046105990869, 0.10758275605023937, 0.10863547666750033, 0.11151596107391606, 0.10324519086235075, 0.10277410007146343, 0.10673169638828467, 0.14111883237958292, 0.09795521786411059, 0.09819758489399746, 0.09881146072384052, 0.10128798761675346, 0.09573852015489476, 0.0963695255869983, 0.10231452211084992, 0.09461537389849246, 0.09495201412085455, 0.09071702625505679, 0.09043751416945565, 0.09169357926220992, 0.0894807253396436, 0.1118812380562453, 0.08894638106857251, 0.08752441168223836, 0.08891461595051078, 0.08568144630947774, 0.0879005813442398, 0.0901965613796091, 0.08374963785689452, 0.08301238641664146, 0.08200517834125966, 0.09080522693221603, 0.08610259625634005, 0.08286054652462055, 0.08387726654409247, 0.0851091272734515, 0.08052692392353361, 0.08608302667585835, 0.08509359827530216, 0.07940412266367172, 0.07981007699494733, 0.07731259425716917, 0.078828237999195, 0.07742086754219438, 0.07637360807699582, 0.07582598891444943, 0.07665929634472946, 0.08056966564735338, 0.0750163516628695, 0.0821035540996595, 0.074323156384213, 0.07363019797966507, 0.07341105152083585, 0.0753426416300231, 0.07451958025877524, 0.08689865447435965, 0.07132267321110893, 0.07134034908908377, 0.0727505356089888, 0.0748779916068292, 0.07354937242847889, 0.07035707044795367, 0.06987088848094729]\n"
     ]
    }
   ],
   "source": [
    "print(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "efe213a5-69e7-4c74-9b33-d880a84345d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9154\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Tải dữ liệu MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Chuyển nhãn thành số nguyên\n",
    "y = y.astype(np.int8)\n",
    "\n",
    "# Chia tập dữ liệu thành train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Chuẩn hóa dữ liệu\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Huấn luyện mô hình Softmax Regression (Logistic Regression đa lớp)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd37b7-6052-4948-b0e4-d429d60563dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
